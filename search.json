[{"path":"https://ercbk.github.io/ebtools/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2021 Eric Book Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://ercbk.github.io/ebtools/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Eric Book. Author, maintainer.","code":""},{"path":"https://ercbk.github.io/ebtools/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Book E (2025). ebtools: Eric's Data Science Toolbox. R package version 0.0.0.9000, https://ercbk.github.io/ebtools.","code":"@Manual{,   title = {ebtools: Eric's Data Science Toolbox},   author = {Eric Book},   year = {2025},   note = {R package version 0.0.0.9000},   url = {https://ercbk.github.io/ebtools}, }"},{"path":"https://ercbk.github.io/ebtools/index.html","id":"erics-data-science-toolbox","dir":"","previous_headings":"","what":"Eric’s Data Science Toolbox","title":"Eric's Data Science Toolbox","text":"personal R package miscellaneous data science functions","code":""},{"path":"https://ercbk.github.io/ebtools/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Eric's Data Science Toolbox","text":"Install GitHub :","code":"# install.packages(\"remotes\") remotes::install_github(\"ercbk/ebtools\")"},{"path":[]},{"path":"https://ercbk.github.io/ebtools/reference/add_mase_scale_feat.html","id":null,"dir":"Reference","previous_headings":"","what":"Add a scale feature by using a factor derived from the MASE error function — add_mase_scale_feat","title":"Add a scale feature by using a factor derived from the MASE error function — add_mase_scale_feat","text":"add_mase_scale_feat() calculates MASE scale factor divides factor group average scale factor produce scale feature.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/add_mase_scale_feat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add a scale feature by using a factor derived from the MASE error function — add_mase_scale_feat","text":"","code":"add_mase_scale_feat(.tbl, .value, ...)"},{"path":"https://ercbk.github.io/ebtools/reference/add_mase_scale_feat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add a scale feature by using a factor derived from the MASE error function — add_mase_scale_feat","text":".tbl tibble; data grouping column value column .value numeric; unquoted name column contains numeric values time series ... character; one unquoted grouping columns","code":""},{"path":"https://ercbk.github.io/ebtools/reference/add_mase_scale_feat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add a scale feature by using a factor derived from the MASE error function — add_mase_scale_feat","text":"original tibble additional column, \"scale.\"","code":""},{"path":"https://ercbk.github.io/ebtools/reference/add_mase_scale_feat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add a scale feature by using a factor derived from the MASE error function — add_mase_scale_feat","text":"Designed use global forecasting method. recommended standardize stacked series used input method. Standardizing stacked series removes scale information series stack might useful generating forecast. Adding scale feature reintroduces information back model.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/add_mase_scale_feat.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Add a scale feature by using a factor derived from the MASE error function — add_mase_scale_feat","text":"Pablo Montero-Manso, Rob J. Hyndman, Principles algorithms forecasting groups time series: Locality globality, International Journal Forecasting, 2021 link","code":""},{"path":"https://ercbk.github.io/ebtools/reference/add_mase_scale_feat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add a scale feature by using a factor derived from the MASE error function — add_mase_scale_feat","text":"","code":"library(dplyr, warn.conflicts = FALSE)  group_ts_tbl <- tsbox::ts_tbl(fpp2::arrivals) #> Registered S3 method overwritten by 'quantmod': #>   method            from #>   as.zoo.data.frame zoo   head(group_ts_tbl) #> # A tibble: 6 × 3 #>   id    time       value #>   <chr> <date>     <dbl> #> 1 Japan 1981-01-01 14.8  #> 2 Japan 1981-04-01  9.32 #> 3 Japan 1981-07-01 10.2  #> 4 Japan 1981-10-01 19.5  #> 5 Japan 1982-01-01 17.1  #> 6 Japan 1982-04-01 10.6   new_tbl <- add_mase_scale_feat(group_ts_tbl, .value = value, id)  head(new_tbl) #> # A tibble: 6 × 4 #>   id    time       value scale #>   <chr> <date>     <dbl> <dbl> #> 1 Japan 1981-01-01 14.8  0.812 #> 2 Japan 1981-04-01  9.32 0.812 #> 3 Japan 1981-07-01 10.2  0.812 #> 4 Japan 1981-10-01 19.5  0.812 #> 5 Japan 1982-01-01 17.1  0.812 #> 6 Japan 1982-04-01 10.6  0.812"},{"path":"https://ercbk.github.io/ebtools/reference/add_spatial_lags.html","id":null,"dir":"Reference","previous_headings":"","what":"Add Spatial Lags of a Variable to a Dataset — add_spatial_lags","title":"Add Spatial Lags of a Variable to a Dataset — add_spatial_lags","text":"Computes spatial lags given numeric variable dataset using neighborhood list spdep package. supports inverse distance weighting, exponential, double power decay weighting methods along various normalization procedures.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/add_spatial_lags.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add Spatial Lags of a Variable to a Dataset — add_spatial_lags","text":"","code":"add_spatial_lags(nblist, y, .data, lags, type = NULL, parallel = FALSE, ...)"},{"path":"https://ercbk.github.io/ebtools/reference/add_spatial_lags.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add Spatial Lags of a Variable to a Dataset — add_spatial_lags","text":"nblist object class \"nb\" spdep package represents neighborhood structure. y character string indicating name numeric variable .data spatial lags computed. .data data frame class \"sf\" containing variable specified y. lags numeric value specifying number spatial lags compute. type character string indicating type spatial weights use. Accepted values \"idw\" (inverse distance weighting), \"exp\" (exponential), \"dpd\" (double power decay), NULL (default), applies standard adjacency-based weighting. parallel (default: FALSE) Logical indicating whether use parallel processing. Requires purrr (>= 1.0.4.9000) installed, mirai (>= 2.1.0.9000) package installed loaded, setting mirai::daemons() number desired processes. (See Examples) ... Additional arguments passed spdep::nb2listw() spdep::nb2listwdist(), style argument specifying normalization method zero-policy indicates whether permit weights list formed zero-length weights vectors.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/add_spatial_lags.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add Spatial Lags of a Variable to a Dataset — add_spatial_lags","text":"tibble containing original dataset additional columns computed spatial lag. spatial lag columns named \"spatlag_<lag>_<y>\". output also includes weight summary attributes named \"summ_wgts_spatlag_<lag>\".","code":""},{"path":"https://ercbk.github.io/ebtools/reference/add_spatial_lags.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add Spatial Lags of a Variable to a Dataset — add_spatial_lags","text":"obtain neighborhood list class \"nb,\" first, neighborhood algorithm fit geometry sf dataset. , object coerced neighborhood list. workflow, see Misc section Geospatial, Spatial Weights note Data Science notebook. Valid Types: (\"idw\", \"exp\", \"dpd\"). Valid Styles: (\"W\", \"B\", \"C\", \"S\", \"U\", \"minmax\", \"raw\"). See Spatial Weights section Geospatial, Spatial Weights note Data Science notebook details spatial weights summary extracted output printing spdep::nb2listw spdep::nb2listwdist object. contains characteristics number regions, number nonzero links, percentage nonzero weights, average number links. n: refers number regions (spatial units) dataset. nn: refers total number possible pairwise relationships regions. calculated n × n. represents total number possible links every region connected every region, including . S0: sum weights. S1: related sum squares weights. S2: related sum products weights pair neighbors. S0, S1, S2 constants used inference global spatial autocorrelation statistics","code":""},{"path":"https://ercbk.github.io/ebtools/reference/add_spatial_lags.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add Spatial Lags of a Variable to a Dataset — add_spatial_lags","text":"","code":"library(spdep, quietly = TRUE) #> To access larger datasets in this package, install the spDataLarge #> package with: `install.packages('spDataLarge', #> repos='https://nowosad.github.io/drat/', type='source')` #> Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE  ny8_sf <-   st_read(system.file(     \"shapes/NY8_bna_utm18.gpkg\",     package = \"spData\"),     quiet = TRUE)  dplyr::glimpse(ny8_sf) #> Rows: 281 #> Columns: 13 #> $ AREAKEY    <chr> \"36007000100\", \"36007000200\", \"36007000300\", \"36007000400\",… #> $ AREANAME   <chr> \"Binghamton city\", \"Binghamton city\", \"Binghamton city\", \"B… #> $ X          <dbl> 4.069397, 4.639371, 5.709063, 7.613831, 7.315968, 8.558753,… #> $ Y          <dbl> -67.3533, -66.8619, -66.9775, -65.9958, -67.3183, -66.9344,… #> $ POP8       <dbl> 3540, 3560, 3739, 2784, 2571, 2729, 3952, 993, 1908, 948, 1… #> $ TRACTCAS   <dbl> 3.08, 4.08, 1.09, 1.07, 3.06, 1.06, 2.09, 0.02, 2.04, 0.02,… #> $ PROPCAS    <dbl> 0.000870, 0.001146, 0.000292, 0.000384, 0.001190, 0.000388,… #> $ PCTOWNHOME <dbl> 0.32773109, 0.42682927, 0.33773959, 0.46160483, 0.19243697,… #> $ PCTAGE65P  <dbl> 0.14661017, 0.23511236, 0.13800481, 0.11889368, 0.14157915,… #> $ Z          <dbl> 0.14197, 0.35555, -0.58165, -0.29634, 0.45689, -0.28123, -0… #> $ AVGIDIST   <dbl> 0.2373852, 0.2087413, 0.1708548, 0.1406045, 0.1577753, 0.17… #> $ PEXPOSURE  <dbl> 3.167099, 3.038511, 2.838229, 2.643366, 2.758587, 2.848411,… #> $ geom       <MULTIPOLYGON [m]> MULTIPOLYGON (((421808.5 46..., MULTIPOLYGON (…  ny8_ct_sf <-   st_centroid(st_geometry(ny8_sf),               of_largest_polygon = TRUE)   ny88_nb_sf <-   knn2nb(knearneigh(ny8_ct_sf,                     k = 4))  # Compute spatial lags tib_spat_lags <-   add_spatial_lags(     nblist = ny88_nb_sf,     y = \"PCTOWNHOME\",     .data = ny8_sf,     lags = 2,     type = \"dpd\",     dmax = 25000,     style = \"W\",     zero.policy = TRUE   )  tib_spat_lags |>   dplyr::select(PCTOWNHOME,                 spatlag_1_PCTOWNHOME,                 spatlag_2_PCTOWNHOME) |>   dplyr::glimpse() #> Rows: 281 #> Columns: 3 #> $ PCTOWNHOME           <dbl> 0.32773109, 0.42682927, 0.33773959, 0.46160483, 0… #> $ spatlag_1_PCTOWNHOME <dbl> 0.3950168, 0.3680243, 0.3577721, 0.3964721, 0.259… #> $ spatlag_2_PCTOWNHOME <dbl> 0.3837656, 0.3191021, 0.3381668, 0.2854097, 0.389…  cat(attributes(tib_spat_lags)$summ_wgts_spatlag_1, sep = \"\\n\") #> Characteristics of weights list object: #> Neighbour list object: #> Number of regions: 281  #> Number of nonzero links: 1124  #> Percentage nonzero weights: 1.423488  #> Average number of links: 4  #> Non-symmetric neighbours list #>  #> Weights style: W  #> Weights constants summary: #>     n    nn  S0     S1       S2 #> W 281 78961 281 117.94 1171.045  rlang::check_installed(   \"mirai (>= 2.1.0.9000)\",   action = function(...) {     remotes::install_version('mirai',                              version = \">= 2.1.0.9000\",                              repos = c('https://shikokuchuo.r-universe.dev',                                        'https://cloud.r-project.org'))   } )  library(mirai)  daemons(2) #> [1] 2  tib_spat_lags_para <-   add_spatial_lags(     nblist = ny88_nb_sf,     y = \"PCTOWNHOME\",     .data = ny8_sf,     lags = 2,     type = \"exp\",     zero.policy = TRUE,     parallel = TRUE   )  daemons(0) #> [1] 0"},{"path":"https://ercbk.github.io/ebtools/reference/add_tsfeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Add time series features calculated by the tsfeatures package — add_tsfeatures","title":"Add time series features calculated by the tsfeatures package — add_tsfeatures","text":"add_tsfeatures() adds set calculated features tsfeatures package time series group. features provide information various characteristics time series.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/add_tsfeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add time series features calculated by the tsfeatures package — add_tsfeatures","text":"","code":"add_tsfeatures(.tbl, ..., standardize = TRUE, parallel = FALSE)"},{"path":"https://ercbk.github.io/ebtools/reference/add_tsfeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add time series features calculated by the tsfeatures package — add_tsfeatures","text":".tbl tibble; data date (class: Date), value (class: numeric), group (class: character) columns ... character; one unquoted grouping columns standardize logical; TRUE (default), function standardize feature. parallel logical; TRUE, features calculated parallel. Default FALSE.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/add_tsfeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add time series features calculated by the tsfeatures package — add_tsfeatures","text":"original tibble 20 additional feature columns.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/add_tsfeatures.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add time series features calculated by the tsfeatures package — add_tsfeatures","text":"Function can used global forecasting method EDA. See tsfeatures website details features.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/add_tsfeatures.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Add time series features calculated by the tsfeatures package — add_tsfeatures","text":"Pablo Montero-Manso, Rob J. Hyndman, Principles algorithms forecasting groups time series: Locality globality, International Journal Forecasting, 2021 link","code":""},{"path":[]},{"path":"https://ercbk.github.io/ebtools/reference/add_tsfeatures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add time series features calculated by the tsfeatures package — add_tsfeatures","text":"","code":"library(dplyr, warn.conflicts = FALSE)  group_ts_tbl <- tsbox::ts_tbl(fpp2::arrivals)  head(group_ts_tbl) #> # A tibble: 6 × 3 #>   id    time       value #>   <chr> <date>     <dbl> #> 1 Japan 1981-01-01 14.8  #> 2 Japan 1981-04-01  9.32 #> 3 Japan 1981-07-01 10.2  #> 4 Japan 1981-10-01 19.5  #> 5 Japan 1982-01-01 17.1  #> 6 Japan 1982-04-01 10.6   new_tbl <- add_tsfeatures(group_ts_tbl, id)  head(new_tbl) #> # A tibble: 6 × 23 #>   id    time       value frequency nperiods seasonal_period trend  spike #>   <chr> <date>     <dbl>     <dbl>    <dbl>           <dbl> <dbl>  <dbl> #> 1 Japan 1981-01-01 14.8          4        1               4 0.327 0.0853 #> 2 Japan 1981-04-01  9.32         4        1               4 0.327 0.0853 #> 3 Japan 1981-07-01 10.2          4        1               4 0.327 0.0853 #> 4 Japan 1981-10-01 19.5          4        1               4 0.327 0.0853 #> 5 Japan 1982-01-01 17.1          4        1               4 0.327 0.0853 #> 6 Japan 1982-04-01 10.6          4        1               4 0.327 0.0853 #> # ℹ 15 more variables: linearity <dbl>, curvature <dbl>, e_acf1 <dbl>, #> #   e_acf10 <dbl>, seasonal_strength <dbl>, peak <dbl>, trough <dbl>, #> #   entropy <dbl>, x_acf1 <dbl>, x_acf10 <dbl>, diff1_acf1 <dbl>, #> #   diff1_acf10 <dbl>, diff2_acf1 <dbl>, diff2_acf10 <dbl>, seas_acf1 <dbl>"},{"path":"https://ercbk.github.io/ebtools/reference/cles.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates the Common Language Effect Size (CLES) — cles","title":"Calculates the Common Language Effect Size (CLES) — cles","text":"Calculates Common Language Effect Size (CLES) two variables. CLES function converts effect size probability unit/subject larger measurement another unit/subject. See Post-Hoc Analysis, Multilevel note Data Science notebook details.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/cles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates the Common Language Effect Size (CLES) — cles","text":"","code":"cles(data, group_variables, paired = FALSE, ci = FALSE, ...)"},{"path":"https://ercbk.github.io/ebtools/reference/cles.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates the Common Language Effect Size (CLES) — cles","text":"data dataframe; Data wide format group_variables character vector list quoted names variables compared. paired boolean; Indicates whether variables correlated repeated measures design. Default FALSE. ci boolean; Indicates whether bootstrap confidence intervals calculated. Default FALSE. ... Additional arguments passed get_boot_ci()","code":""},{"path":"https://ercbk.github.io/ebtools/reference/cles.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculates the Common Language Effect Size (CLES) — cles","text":"'ci = FALSE', function returns scalar value estimate CLES. 'ci = TRUE', function returns dataframe following columns: ci_type: method calculating bootstrap confidence intervals. conf: confidence level bootstrap confidence intervals, .lower: lower value bootstrap confidence interval. .estimate: CLES point estimate. .upper: upper value bootstrap confidence interval.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/cles.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculates the Common Language Effect Size (CLES) — cles","text":"measure also referred Probability Superiority. conversion effect size probability percentage supposed easier laymen interpret. Interpretation: -Subjects: probability randomly sampled person one group higher observed measurement randomly sampled person group. Within-Subjects: probability individual higher value one measurement . -Subjects Formula: $$\\tilde d = \\frac{|M_1 - M_2|}{\\sqrt{p_1\\text{SD}_1^2 + p_2\\text{SD}_2^2}}\\\\ Z = \\frac{\\tilde d}{\\sqrt{2}}$$ \\(M_i\\): mean ith group \\(p_i\\): proportion sample size ith group \\(Z\\): z-score turn used produce probability. Within-Subjects Formula: $$Z = \\frac{|M_1 - M_2|}{\\sqrt{\\operatorname{SD}_1^2 + \\operatorname{SD}_2^2 - 2 \\times r \\times \\operatorname{SD}_1 \\times \\operatorname{SD}_2}}$$ \\(M_i\\): mean ith group \\(r\\): Pearson correlation two variables \\(Z\\): z-score turn used produce probability.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/cles.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculates the Common Language Effect Size (CLES) — cles","text":"McGraw, K. O., & Wong, S. P. (1992). common language effect size statistic. Psychological Bulletin, 111(2), 361–365. https://doi.org/10.1037/0033-2909.111.2.361","code":""},{"path":"https://ercbk.github.io/ebtools/reference/cles.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculates the Common Language Effect Size (CLES) — cles","text":"","code":"movie_dat <- dplyr::tibble(    movie1 = c(9.00, 7.00, 8.00, 9.00, 8.00, 9.00, 9.00, 10.00, 9.00, 9.00),    movie2 = c(9.00, 6.00, 7.00, 8.00, 7.00, 9.00, 8.00, 8.00, 8.00, 7.00) )  # between-subjects design cles(data = movie_dat,      group_variables = list(\"movie1\", \"movie2\")) #> [1] 0.7870181  # within-subjects design and bootstrap CIs cles(data = movie_dat,      group_variables = list(\"movie1\", \"movie2\"),      paired = TRUE,      ci = TRUE,      R = 10000,      type = c(\"bca\", \"perc\")) #>   ci_type conf .lower .estimate .upper #> 1 percent 0.95 0.8080 0.9331928 0.9997 #> 2     bca 0.95 0.7602 0.9331928 0.9964"},{"path":"https://ercbk.github.io/ebtools/reference/create_dtw_grids.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a parameter grid list for dtwclust and dtw distance functions — create_dtw_grids","title":"Create a parameter grid list for dtwclust and dtw distance functions — create_dtw_grids","text":"create_dtw_grids() creates nested, parameter grid list dtwclust dtw distance functions used dtw_dist_gridsearch().","code":""},{"path":"https://ercbk.github.io/ebtools/reference/create_dtw_grids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a parameter grid list for dtwclust and dtw distance functions — create_dtw_grids","text":"","code":"create_dtw_grids(params)"},{"path":"https://ercbk.github.io/ebtools/reference/create_dtw_grids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a parameter grid list for dtwclust and dtw distance functions — create_dtw_grids","text":"params named list parameter name-value pairs. names names distance functions correspond parameter name-value pairs.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/create_dtw_grids.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a parameter grid list for dtwclust and dtw distance functions — create_dtw_grids","text":"Named, nested list element list elements represent possible configuration parameters distance function.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/create_dtw_grids.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a parameter grid list for dtwclust and dtw distance functions — create_dtw_grids","text":"","code":"params_ls_lg <- list(dtw_basic = list(window.size = 5:10,                                       norm = c(\"L1\", \"L2\"),                                       step.pattern = list(dtw::symmetric1, dtw::symmetric2)),                      dtw2 = list(step.pattern = list(dtw::symmetric1, dtw::symmetric2),                                  window.size = 5:10),                      dtw_lb = list(window.size = 5:10,                                    norm = c(\"L1\", \"L2\"),                                    dtw.func = \"dtw_basic\",                                    step.pattern = list(dtw::symmetric2)),                      sbd = list(znorm = TRUE, return.shifted = FALSE),                      gak = list(normalize = TRUE, window.size = 5:10))  dtw_grids_lg <- create_dtw_grids(params_ls_lg) str(dtw_grids_lg$dtw_basic[1:4]) #> List of 4 #>  $ :List of 4 #>   ..$ window.size    : int 5 #>   ..$ norm           : chr \"L1\" #>   ..$ step.pattern   : 'stepPattern' num [1:6, 1:4] 1 1 2 2 3 3 1 0 0 0 ... #>   .. ..- attr(*, \"npat\")= num 3 #>   .. ..- attr(*, \"norm\")= logi NA #>   ..$ step_pattern_id: chr \"symmetric1\" #>  $ :List of 4 #>   ..$ window.size    : int 6 #>   ..$ norm           : chr \"L1\" #>   ..$ step.pattern   : 'stepPattern' num [1:6, 1:4] 1 1 2 2 3 3 1 0 0 0 ... #>   .. ..- attr(*, \"npat\")= num 3 #>   .. ..- attr(*, \"norm\")= logi NA #>   ..$ step_pattern_id: chr \"symmetric1\" #>  $ :List of 4 #>   ..$ window.size    : int 7 #>   ..$ norm           : chr \"L1\" #>   ..$ step.pattern   : 'stepPattern' num [1:6, 1:4] 1 1 2 2 3 3 1 0 0 0 ... #>   .. ..- attr(*, \"npat\")= num 3 #>   .. ..- attr(*, \"norm\")= logi NA #>   ..$ step_pattern_id: chr \"symmetric1\" #>  $ :List of 4 #>   ..$ window.size    : int 8 #>   ..$ norm           : chr \"L1\" #>   ..$ step.pattern   : 'stepPattern' num [1:6, 1:4] 1 1 2 2 3 3 1 0 0 0 ... #>   .. ..- attr(*, \"npat\")= num 3 #>   .. ..- attr(*, \"norm\")= logi NA #>   ..$ step_pattern_id: chr \"symmetric1\"   # Can still be ran with a minimal \"grid\" params_ls_sm <- list(dtw2 = list(step.pattern = list(dtw::symmetric1)))  dtw_grids_sm <- create_dtw_grids(params_ls_sm) head(dtw_grids_sm) #> $dtw2 #> $dtw2[[1]] #> $dtw2[[1]]$step.pattern #> Step pattern recursion: #> g[i,j] = min( #>      g[i-1,j-1] +     d[i  ,j  ] , #>      g[i  ,j-1] +     d[i  ,j  ] , #>      g[i-1,j  ] +     d[i  ,j  ] , #>   ) #>  #>  Normalization hint: NA #>  #> $dtw2[[1]]$step_pattern_id #> [1] \"symmetric1\" #>  #>  #>"},{"path":"https://ercbk.github.io/ebtools/reference/descale_by_mase.html","id":null,"dir":"Reference","previous_headings":"","what":"Back-transform a MASE-scaled column — descale_by_mase","title":"Back-transform a MASE-scaled column — descale_by_mase","text":"descale_by_mase() back-transforms group time series scaled factor derived MASE error function.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/descale_by_mase.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Back-transform a MASE-scaled column — descale_by_mase","text":"","code":"descale_by_mase(.tbl, .value, scale_factors, ...)"},{"path":"https://ercbk.github.io/ebtools/reference/descale_by_mase.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Back-transform a MASE-scaled column — descale_by_mase","text":".tbl tibble; data value (class: numeric) column group (class: character) column(s) .value numeric; unquoted name column contains scaled numeric values scale_factors tibble; tibble extracted scale_factors attribute output scale_by_mase(). Grouping columns match .tbl. (See details) ... character; one unquoted grouping columns","code":""},{"path":"https://ercbk.github.io/ebtools/reference/descale_by_mase.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Back-transform a MASE-scaled column — descale_by_mase","text":"original tibble .value column back-transformed orginal scale.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/descale_by_mase.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Back-transform a MASE-scaled column — descale_by_mase","text":"Scaling grouped time series can helpful global forecasting methods using machine learning deep learning algorithms. Scaling MASE using MASE error function equivalent minimizing MAE preprocessed time series. scale_factors tibble can extracted scale_factors <- attributes(mase_scaled_tbl)$scale_factors mase_scaled_tbl output scale_by_mase().","code":""},{"path":"https://ercbk.github.io/ebtools/reference/descale_by_mase.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Back-transform a MASE-scaled column — descale_by_mase","text":"Pablo Montero-Manso, Rob J. Hyndman, Principles algorithms forecasting groups time series: Locality globality, International Journal Forecasting, 2021 link","code":""},{"path":"https://ercbk.github.io/ebtools/reference/descale_by_mase.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Back-transform a MASE-scaled column — descale_by_mase","text":"","code":"library(dplyr, warn.conflicts = FALSE)  group_ts_tbl <- tsbox::ts_tbl(fpp2::arrivals)  head(group_ts_tbl) #> # A tibble: 6 × 3 #>   id    time       value #>   <chr> <date>     <dbl> #> 1 Japan 1981-01-01 14.8  #> 2 Japan 1981-04-01  9.32 #> 3 Japan 1981-07-01 10.2  #> 4 Japan 1981-10-01 19.5  #> 5 Japan 1982-01-01 17.1  #> 6 Japan 1982-04-01 10.6   new_tbl <- scale_by_mase(.tbl = group_ts_tbl, .value = value, id)  glimpse(new_tbl) #> Rows: 508 #> Columns: 3 #> $ id    <chr> \"Japan\", \"Japan\", \"Japan\", \"Japan\", \"Japan\", \"Japan\", \"Japan\", \"… #> $ time  <date> 1981-01-01, 1981-04-01, 1981-07-01, 1981-10-01, 1982-01-01, 198… #> $ value <dbl> 0.7221063, 0.4559204, 0.4972521, 0.9542486, 0.8372481, 0.5193120…  scale_factors <- attributes(new_tbl)$scale_factors  orig_tbl <- descale_by_mase(new_tbl, value, scale_factors, id)  head(orig_tbl) #> # A tibble: 6 × 3 #>   id    time       value #>   <chr> <date>     <dbl> #> 1 Japan 1981-01-01 14.8  #> 2 Japan 1981-04-01  9.32 #> 3 Japan 1981-07-01 10.2  #> 4 Japan 1981-10-01 19.5  #> 5 Japan 1982-01-01 17.1  #> 6 Japan 1982-04-01 10.6"},{"path":"https://ercbk.github.io/ebtools/reference/dtw_dist_gridsearch.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform a grid search of parameters and dtwclust distance functions — dtw_dist_gridsearch","title":"Perform a grid search of parameters and dtwclust distance functions — dtw_dist_gridsearch","text":"dtw_dist_gridsearch() performs gridsearch using list parameter grids list distance functions dtwclust package.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/dtw_dist_gridsearch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform a grid search of parameters and dtwclust distance functions — dtw_dist_gridsearch","text":"","code":"dtw_dist_gridsearch(   query_tbl,   ref_series,   dtw_funs,   dtw_grids,   num_best = \"all\" )"},{"path":"https://ercbk.github.io/ebtools/reference/dtw_dist_gridsearch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform a grid search of parameters and dtwclust distance functions — dtw_dist_gridsearch","text":"query_tbl Data.frame tibble containing columns numeric vectors query time series compared reference time series. ref_series Numeric vector; reference time series series query series compared . dtw_funs Named list dtwclust distance functions. Names need match dtw_grids dtw_grids Object created create_dtw_grids() named nested list parameter name-value pairs correspond distance functions. Names need match dtw_funs. num_best Integer \"\"; integer, number query series lowest distance values parameter configuration returned; \"\", results returned. Default \"\".","code":""},{"path":"https://ercbk.github.io/ebtools/reference/dtw_dist_gridsearch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform a grid search of parameters and dtwclust distance functions — dtw_dist_gridsearch","text":"tibble columns names query series, names distance functions, parameter values, calculated distances.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/dtw_dist_gridsearch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Perform a grid search of parameters and dtwclust distance functions — dtw_dist_gridsearch","text":"distance algorithms currently supported : dynamic time warping (dtw_basic) dynamic time warping additional L2 Norm (dtw2) dynamic time warping lower bound (dtw_lb) Triangular Global Alignment Kernel (gak) Slope Based Distance (sbd)","code":""},{"path":[]},{"path":"https://ercbk.github.io/ebtools/reference/dtw_dist_gridsearch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform a grid search of parameters and dtwclust distance functions — dtw_dist_gridsearch","text":"","code":"suppressPackageStartupMessages(library(dtwclust))  head(ohio_covid)[,1:6] #> # A tibble: 6 × 6 #>   date       cases deaths_lead60 deaths_lead59 deaths_lead58 deaths_lead57 #>   <date>     <dbl>         <dbl>         <dbl>         <dbl>         <dbl> #> 1 2020-03-22  44.9          43.1          42.6          40.6          42.9 #> 2 2020-03-23  56.3          41.6          43.1          42.6          40.6 #> 3 2020-03-24  71            49.4          41.6          43.1          42.6 #> 4 2020-03-25  88.1          49.1          49.4          41.6          43.1 #> 5 2020-03-26 107.           47.1          49.1          49.4          41.6 #> 6 2020-03-27 139.           40.3          47.1          49.1          49.4  ref_series <- ohio_covid[[\"cases\"]] query_tbl <- dplyr::select(ohio_covid, -cases, -date)   params_ls_lg <- list(dtw_basic = list(window.size = 5:10,                                       norm = c(\"L1\", \"L2\"),                                       step.pattern = list(symmetric1, symmetric2)),                      dtw2 = list(step.pattern = list(symmetric1, symmetric2),                                  window.size = 5:10),                      dtw_lb = list(window.size = 5:10,                                    norm = c(\"L1\", \"L2\"),                                    dtw.func = \"dtw_basic\",                                    step.pattern = list(symmetric2)),                      sbd = list(znorm = TRUE, return.shifted = FALSE),                      gak = list(normalize = TRUE, window.size = 5:10))  dtw_grids_lg <- create_dtw_grids(params_ls_lg)  dtw_funs_lg <- list(dtw_basic = dtw_basic,                     dtw2 = dtw2,                     dtw_lb = dtw_lb,                     sbd = sbd,                     gak = gak)  search_res_lg <- dtw_dist_gridsearch(query_tbl = query_tbl,                                      ref_series = ref_series,                                      dtw_funs = dtw_funs_lg,                                      dtw_grids = dtw_grids_lg,                                      num_best = 2)  head(search_res_lg) #> # A tibble: 6 × 10 #>   query  algorithm distance window.size step_pattern_id norm  dtw.func normalize #>   <chr>  <chr>        <dbl>       <int> <chr>           <chr> <chr>    <lgl>     #> 1 death… dtw2        39791.           5 symmetric1      NA    NA       NA        #> 2 death… dtw2        39791.           5 symmetric1      NA    NA       NA        #> 3 death… dtw2        39791.           5 symmetric2      NA    NA       NA        #> 4 death… dtw2        39793.           5 symmetric2      NA    NA       NA        #> 5 death… dtw2        39791.           6 symmetric1      NA    NA       NA        #> 6 death… dtw2        39791.           6 symmetric1      NA    NA       NA        #> # ℹ 2 more variables: znorm <lgl>, return.shifted <lgl>   # Can still be ran with a minimal \"grid\" params_ls_sm <- list(dtw2 = list(step.pattern = list(symmetric1)))  dtw_grids_sm <- create_dtw_grids(params_ls_sm)  dtw_funs_sm <- list(dtw2 = dtw2)  search_res_sm <- dtw_dist_gridsearch(query_tbl = query_tbl,                                      ref_series = ref_series,                                      dtw_funs = dtw_funs_sm,                                      dtw_grids = dtw_grids_sm,                                      num_best = \"all\")  head(search_res_sm) #> # A tibble: 6 × 4 #>   query         algorithm distance step_pattern_id #>   <chr>         <chr>        <dbl> <chr>           #> 1 deaths_lead60 dtw2        39838. symmetric1      #> 2 deaths_lead59 dtw2        39834. symmetric1      #> 3 deaths_lead58 dtw2        39831. symmetric1      #> 4 deaths_lead57 dtw2        39828. symmetric1      #> 5 deaths_lead56 dtw2        39827. symmetric1      #> 6 deaths_lead55 dtw2        39825. symmetric1"},{"path":"https://ercbk.github.io/ebtools/reference/get_boot_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates bootstrapped confidence intervals for a statistic — get_boot_ci","title":"Calculates bootstrapped confidence intervals for a statistic — get_boot_ci","text":"get_boot_ci() wraps boot boot.ci boot package, uses sensible argument values, returns dataframe confidence interval(s) point estimate.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/get_boot_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates bootstrapped confidence intervals for a statistic — get_boot_ci","text":"","code":"get_boot_ci(   data,   stat_fun,   conf = 0.95,   type = \"bca\",   R = 1000,   parallel = \"windows\",   add_boot = NULL,   add_boot_ci = NULL,   ... )"},{"path":"https://ercbk.github.io/ebtools/reference/get_boot_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates bootstrapped confidence intervals for a statistic — get_boot_ci","text":"data dataframe; Data variables required function calculates statistic interest. stat_fun function; function calculates statistic interest. conf scalar numeric vector; confidence level(s) required interval(s).default 0.95. type string character vector; vector character strings representing type intervals required. value subset values c(\"norm\",\"basic\", \"stud\", \"perc\", \"bca\") simply \"\" compute five types intervals. Default \"bca\". R scalar; number bootstrap replicates intervals based. 1000 default. parallel string; value either \"windows\", \"\", \"\". type operating system determines method parallelization. \"windows\" default indicates Microsoft Windows. Mac Linux, \"\" used. \"\" indicates calculations parallelized. add_boot list; list additional argument value pair(s) included boot set arguments. See boot package documentation details. add_boot_ci list; list additional argument value pair(s) included boot.ci set arguments. See boot package documentation details. ... Arguments values required 'stat_fun'","code":""},{"path":"https://ercbk.github.io/ebtools/reference/get_boot_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculates bootstrapped confidence intervals for a statistic — get_boot_ci","text":"dataframe following columns: type: type confidence interval calculated conf: confidence level calculated intervals .lower: lower value confidence interval .upper: upper value confidence interval point estimate statistic included attributes dataframe.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/get_boot_ci.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculates bootstrapped confidence intervals for a statistic — get_boot_ci","text":"boot boot.ci functions boot package large number options (together), can bit overwhelming just want bootstrap CIs quickly. tried simplify choices function also maintaining flexibility add options complex cases. user must adapt function using calculate statistic interest ('stat_fun') include necessary argument according chosen resampling option. default resampling option \"indices\" ('stype = \"\"'), one going elaborate (See examples \"weights\" option. See boot package documentation details \"frequency\" option). order use option, user must: Include index argument statistic function, must second argument (data argument first). Use argument either subset rows data variable(s) body function. Examples illustrate procedure.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/get_boot_ci.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculates bootstrapped confidence intervals for a statistic — get_boot_ci","text":"Canty , Ripley BD (2022). boot: Bootstrap R (S-Plus) Functions. R package version 1.3-28.1. Davison AC, Hinkley DV (1997). Bootstrap Methods Applications. Cambridge University Press, Cambridge. ISBN 0-521-57391-2, http://statwww.epfl.ch/davison/BMA/.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/get_boot_ci.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculates bootstrapped confidence intervals for a statistic — get_boot_ci","text":"","code":"# weights resampling option, d is the data, w is the weight data(city, package = \"boot\") ratio <- function(d, w) sum(d$x * w)/sum(d$u * w) get_boot_ci(   data = city,   stat_fun = ratio,   add_boot = list(stype = \"w\") ) #>   type conf .lower .upper #> 1  bca 0.95 1.2588 2.1934  # indices used on variable example, d is data, i is index data(aircondit, package = \"boot\") mean.fun <- function(d, i) {   m <- mean(d$hours[i])   n <- length(i)   v <- (n-1)*var(d$hours[i])/n^2   c(m, v) } get_boot_ci(   data = aircondit,   stat_fun = mean.fun, ) #>   type conf  .lower  .upper #> 1  bca 0.95 55.8018 222.526  # indices used on data object example movie_dat <- dplyr::tibble(   movie1 = c(9.00, 7.00, 8.00, 9.00, 8.00, 9.00, 9.00, 10.00, 9.00, 9.00),   movie2 = c(9.00, 6.00, 7.00, 8.00, 7.00, 9.00, 8.00, 8.00, 8.00, 7.00) )  movie_dat_long <- movie_dat |>  tidyr::pivot_longer(cols = c(movie1, movie2),                      names_to = \"movies\",                      values_to =\"ratings\")  # \"ind\" is the index argument and is used to subset the data cles_boot <- function(data, ind, variable, group, baseline) {    dat <- data[ind, ]    # Select the observations for group 1   x <- dat[dat[[group]] == baseline, variable][[1]]    # Select the observations for group 2   y <- dat[dat[[group]] != baseline, variable][[1]]    # Variances will be weighted by each group's proportion of the sample size   p1 <- length(x)/(length(x) + length(y))   p2 <- length(y)/(length(x) + length(y))    # Mean difference between x and y   diff <- abs(mean(x) - mean(y))    # Standard deviation of difference   standardizer <- sqrt((p1*sd(x)^2 + p2*sd(y)^2))    z_score <- (diff/standardizer)/sqrt(2)    # Probability derived from normal distribution   # that random x is higher than random y -   # or in other words, that diff is larger than 0.   prob_norm <- pnorm(z_score)    # Return result   return(prob_norm) }  results <-    get_boot_ci(      data = movie_dat_long,      stat_fun = cles_boot,      type = c(\"perc\", \"bca\"),      conf = c(0.80, 0.95),      parallel = \"no\",      variable = \"ratings\",      group = \"movies\",      baseline = \"movie1\"    ) results #>      type conf .lower .upper #> 1 percent 0.80 0.6527 0.9083 #> 2 percent 0.95 0.5609 0.9605 #> 3     bca 0.80 0.6115 0.8879 #> 4     bca 0.95 0.5256 0.9330 attributes(results)$estimate #> [1] 0.7870181"},{"path":"https://ercbk.github.io/ebtools/reference/indiana_pos_rate.html","id":null,"dir":"Reference","previous_headings":"","what":"Indiana COVID-19 Positivity Rates Data Set — indiana_pos_rate","title":"Indiana COVID-19 Positivity Rates Data Set — indiana_pos_rate","text":"Contains weekly COVID-19 positivity rates metropolitan statistical areas Indiana April 4th, 2020 January 30th, 2021.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/indiana_pos_rate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Indiana COVID-19 Positivity Rates Data Set — indiana_pos_rate","text":"","code":"indiana_pos_rate"},{"path":"https://ercbk.github.io/ebtools/reference/indiana_pos_rate.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Indiana COVID-19 Positivity Rates Data Set — indiana_pos_rate","text":"object class tbl_df (inherits tbl, data.frame) 528 rows 3 columns.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/indiana_pos_rate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Indiana COVID-19 Positivity Rates Data Set — indiana_pos_rate","text":"Data collected state health departments curated Indiana COVIDcast Dashboard repository.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/indiana_pos_rate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Indiana COVID-19 Positivity Rates Data Set — indiana_pos_rate","text":"","code":"head(indiana_pos_rate) #> # A tibble: 6 × 3 #>   end_date   msa         pos_rate #>   <date>     <chr>          <dbl> #> 1 2020-04-04 Bloomington   0.112  #> 2 2020-04-11 Bloomington   0.125  #> 3 2020-04-18 Bloomington   0.111  #> 4 2020-04-25 Bloomington   0.0281 #> 5 2020-05-02 Bloomington   0.0232 #> 6 2020-05-09 Bloomington   0.0233"},{"path":"https://ercbk.github.io/ebtools/reference/ohio_covid.html","id":null,"dir":"Reference","previous_headings":"","what":"Ohio COVID-19 Data Set — ohio_covid","title":"Ohio COVID-19 Data Set — ohio_covid","text":"Contains rolling 7-day averages positive COVID-19 cases leads COVID-19 deaths Ohio March 22nd, 2020 December 1st, 2020.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/ohio_covid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ohio COVID-19 Data Set — ohio_covid","text":"","code":"ohio_covid"},{"path":"https://ercbk.github.io/ebtools/reference/ohio_covid.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Ohio COVID-19 Data Set — ohio_covid","text":"object class tbl_df (inherits tbl, data.frame) 255 rows 63 columns.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/ohio_covid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ohio COVID-19 Data Set — ohio_covid","text":"Data collected New York Times COVID-19 data repository","code":""},{"path":"https://ercbk.github.io/ebtools/reference/ohio_covid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Ohio COVID-19 Data Set — ohio_covid","text":"","code":"head(ohio_covid)[,1:6] #> # A tibble: 6 × 6 #>   date       cases deaths_lead60 deaths_lead59 deaths_lead58 deaths_lead57 #>   <date>     <dbl>         <dbl>         <dbl>         <dbl>         <dbl> #> 1 2020-03-22  44.9          43.1          42.6          40.6          42.9 #> 2 2020-03-23  56.3          41.6          43.1          42.6          40.6 #> 3 2020-03-24  71            49.4          41.6          43.1          42.6 #> 4 2020-03-25  88.1          49.1          49.4          41.6          43.1 #> 5 2020-03-26 107.           47.1          49.1          49.4          41.6 #> 6 2020-03-27 139.           40.3          47.1          49.1          49.4"},{"path":"https://ercbk.github.io/ebtools/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://ercbk.github.io/ebtools/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://ercbk.github.io/ebtools/reference/prewhitened_ccf.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate cross-correlation coefficients for prewhitened time series — prewhitened_ccf","title":"Calculate cross-correlation coefficients for prewhitened time series — prewhitened_ccf","text":"prewhitened_ccf() prewhitens time series, calculates cross-correlation coefficients, returns statistically significant values.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/prewhitened_ccf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate cross-correlation coefficients for prewhitened time series — prewhitened_ccf","text":"","code":"prewhitened_ccf(   input,   output,   input_col,   output_col,   keep_input = \"both\",   keep_ccf = \"both\",   max.order )"},{"path":"https://ercbk.github.io/ebtools/reference/prewhitened_ccf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate cross-correlation coefficients for prewhitened time series — prewhitened_ccf","text":"input tsibble; influential \"predictor-like\" time series output tsibble; affected \"response-like\" time series input_col string; Name numeric column input tsibble output_col string; Name numeric column output tsibble keep_input string; values: \"input_lags\", \"input_leads\" \"\"; Default \"\". keep_ccf string; values: \"positive\", \"negative\", \"; Default \"\" max.order integer; maximum lag used CCF calculation.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/prewhitened_ccf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate cross-correlation coefficients for prewhitened time series — prewhitened_ccf","text":"tibble following columns: input_type: \"lag\" \"lead\" input_series: lag lead number signif_type: \"Statistically Significant\" \"Statistically Significant\" signif_threshold: Threashold CCF value statistical significance 95% level ccf: Calculated ccf value","code":""},{"path":"https://ercbk.github.io/ebtools/reference/prewhitened_ccf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate cross-correlation coefficients for prewhitened time series — prewhitened_ccf","text":"cross-correlation direction influence two time-series hypothesized known, influential time-series called \"input\" time-series affected time-series called \"output\" time-series cross-correlation function calculates correlation values lags leads input series output series. Sometimes correlations leads lags input series output series make theoretical sense, positive negative correlations make theoretical sense. \"keep_input\" argument specifies whether want keep output CCF values involving leads lags input series . \"keep_ccf\" argument specifies whether want keep output positive, negative, CCF values. prewhitened_ccf differences series needed, prewhitens, outputs either statistically significant values CCF top non-statistically significant value statistically significant values found. prewhitening method used Cryer Chan (2008, Chapter 11).","code":""},{"path":"https://ercbk.github.io/ebtools/reference/prewhitened_ccf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate cross-correlation coefficients for prewhitened time series — prewhitened_ccf","text":"Cryer, D., Chan, K. (2008) Time Series Analysis, Springer Science+Business Media, LLC","code":""},{"path":"https://ercbk.github.io/ebtools/reference/prewhitened_ccf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate cross-correlation coefficients for prewhitened time series — prewhitened_ccf","text":"","code":"oh_cases <- ohio_covid %>%    dplyr::select(date, cases) %>%    tsibble::as_tsibble(index = date) #> Registered S3 method overwritten by 'tsibble': #>   method               from  #>   as_tibble.grouped_df dplyr  oh_deaths <- ohio_covid %>%    dplyr::select(date, deaths_lead0) %>%    tsibble::as_tsibble(index = date)  oh_ccf_tbl <- prewhitened_ccf(input = oh_cases,                               output = oh_deaths,                               input_col = \"cases\",                               output_col = \"deaths_lead0\",                               max.order = 40,                               keep_input = \"input_lag\",                               keep_ccf = \"positive\")  oh_ccf_tbl #> # A tibble: 1 × 5 #>   input_type input_series signif_type                   signif_thresh    ccf #>   <chr>             <dbl> <chr>                                 <dbl>  <dbl> #> 1 lag                   6 Not Statistically Significant         0.123 0.0772   library(dplyr, warn.conflicts = FALSE)  reg_cases_tsb <- us_regional_cases %>%   tsibble::as_tsibble(index = date, key = region) %>%   tsibble::group_by_key() %>%   tidyr::nest() %>%   arrange(region) %>%   ungroup() %>%   mutate(id = as.character(row_number()))  reg_deaths_tsb <- us_regional_deaths %>%   tsibble::as_tsibble(index = date, key = region) %>%   tsibble::group_by_key() %>%   tidyr::nest() %>%   arrange(region) %>%   ungroup() %>%   mutate(id = as.character(row_number()))  reg_ccf_vals <- purrr::map2_dfr(reg_cases_tsb$data,                                 reg_deaths_tsb$data,                                 prewhitened_ccf,                                 input_col = \"reg_sev_day_cases\",                                 output_col = \"reg_sev_day_deaths\",                                 max.order = 40,                                 .id = \"id\") %>%   left_join(reg_cases_tsb %>%              select(id, region), by = \"id\")  head(reg_ccf_vals) #> # A tibble: 6 × 7 #>   id    input_type input_series signif_type          signif_thresh    ccf region #>   <chr> <chr>             <dbl> <chr>                        <dbl>  <dbl> <chr>  #> 1 1     lag                  28 Statistically Signi…         0.110  0.132 midwe… #> 2 1     lag                  21 Statistically Signi…        -0.110 -0.137 midwe… #> 3 1     lag                  19 Statistically Signi…         0.110  0.110 midwe… #> 4 1     lag                  18 Statistically Signi…         0.110  0.125 midwe… #> 5 1     lag                  14 Statistically Signi…        -0.110 -0.110 midwe… #> 6 1     lag                   0 Statistically Signi…         0.110  0.222 midwe…"},{"path":"https://ercbk.github.io/ebtools/reference/scale_by_mase.html","id":null,"dir":"Reference","previous_headings":"","what":"Scale a group time series by using a factor derived from the MASE error function — scale_by_mase","title":"Scale a group time series by using a factor derived from the MASE error function — scale_by_mase","text":"scale_by_mase() scales group time series using factor derived MASE error function.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/scale_by_mase.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scale a group time series by using a factor derived from the MASE error function — scale_by_mase","text":"","code":"scale_by_mase(.tbl, .value, ...)"},{"path":"https://ercbk.github.io/ebtools/reference/scale_by_mase.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scale a group time series by using a factor derived from the MASE error function — scale_by_mase","text":".tbl tibble; data value (class: numeric) column group (class: character) column(s) .value numeric; unquoted name column contains numeric values ... character; one unquoted grouping columns","code":""},{"path":"https://ercbk.github.io/ebtools/reference/scale_by_mase.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scale a group time series by using a factor derived from the MASE error function — scale_by_mase","text":"original tibble .value column back-transformed orginal scale.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/scale_by_mase.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Scale a group time series by using a factor derived from the MASE error function — scale_by_mase","text":"Scaling grouped time series can helpful global forecasting methods using machine learning deep learning algorithms. Scaling MASE using MASE error function equivalent minimizing MAE preprocessed time series. series, MASE scale factor calculated using denominator MASE scaled error equation. , series divided factor.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/scale_by_mase.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Scale a group time series by using a factor derived from the MASE error function — scale_by_mase","text":"Pablo Montero-Manso, Rob J. Hyndman, Principles algorithms forecasting groups time series: Locality globality, International Journal Forecasting, 2021 link","code":""},{"path":"https://ercbk.github.io/ebtools/reference/scale_by_mase.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scale a group time series by using a factor derived from the MASE error function — scale_by_mase","text":"","code":"library(dplyr, warn.conflicts = FALSE)  group_ts_tbl <- tsbox::ts_tbl(fpp2::arrivals)  head(group_ts_tbl) #> # A tibble: 6 × 3 #>   id    time       value #>   <chr> <date>     <dbl> #> 1 Japan 1981-01-01 14.8  #> 2 Japan 1981-04-01  9.32 #> 3 Japan 1981-07-01 10.2  #> 4 Japan 1981-10-01 19.5  #> 5 Japan 1982-01-01 17.1  #> 6 Japan 1982-04-01 10.6   new_tbl <- scale_by_mase(.tbl = group_ts_tbl, .value = value, id)  head(new_tbl) #> # A tibble: 6 × 3 #>   id    time       value #>   <chr> <date>     <dbl> #> 1 Japan 1981-01-01 0.722 #> 2 Japan 1981-04-01 0.456 #> 3 Japan 1981-07-01 0.497 #> 4 Japan 1981-10-01 0.954 #> 5 Japan 1982-01-01 0.837 #> 6 Japan 1982-04-01 0.519  attributes(new_tbl)$scale_factors #> # A tibble: 4 × 2 #>   id    scale #>   <chr> <dbl> #> 1 Japan 20.4  #> 2 NZ    33.0  #> 3 UK    37.5  #> 4 US     9.69"},{"path":"https://ercbk.github.io/ebtools/reference/test_fable_resids.html","id":null,"dir":"Reference","previous_headings":"","what":"Autocorrelation test of the residuals of dynamic regression fable models with various specifications that have been fitted for a grouping variable — test_fable_resids","title":"Autocorrelation test of the residuals of dynamic regression fable models with various specifications that have been fitted for a grouping variable — test_fable_resids","text":"test_fable_resids() takes nested tibble checks fable model residuals autocorrelation using Ljung-Box test.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/test_fable_resids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Autocorrelation test of the residuals of dynamic regression fable models with various specifications that have been fitted for a grouping variable — test_fable_resids","text":"","code":"test_fable_resids(mod_tbl, grp_col, mod_col)"},{"path":"https://ercbk.github.io/ebtools/reference/test_fable_resids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Autocorrelation test of the residuals of dynamic regression fable models with various specifications that have been fitted for a grouping variable — test_fable_resids","text":"mod_tbl tibble grouping variable nested list column list model objects grouping variable value grp_col name grouping variable column mod_col name nested list column lists fable model objects","code":""},{"path":"https://ercbk.github.io/ebtools/reference/test_fable_resids.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Autocorrelation test of the residuals of dynamic regression fable models with various specifications that have been fitted for a grouping variable — test_fable_resids","text":"unnested tibble columns grouping variable, model names, p-values Ljung-Box test.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/test_fable_resids.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Autocorrelation test of the residuals of dynamic regression fable models with various specifications that have been fitted for a grouping variable — test_fable_resids","text":"P-values less 0.05 indicate autocorrelation present. p-values round less 0.000, single \"0\" returned.","code":""},{"path":[]},{"path":"https://ercbk.github.io/ebtools/reference/test_fable_resids.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Autocorrelation test of the residuals of dynamic regression fable models with various specifications that have been fitted for a grouping variable — test_fable_resids","text":"","code":"library(dplyr, warn.conflicts = FALSE) library(fable, quietly = TRUE) library(furrr, quietly = TRUE) plan(multisession)  head(ohio_covid)[,1:6] #> # A tibble: 6 × 6 #>   date       cases deaths_lead60 deaths_lead59 deaths_lead58 deaths_lead57 #>   <date>     <dbl>         <dbl>         <dbl>         <dbl>         <dbl> #> 1 2020-03-22  44.9          43.1          42.6          40.6          42.9 #> 2 2020-03-23  56.3          41.6          43.1          42.6          40.6 #> 3 2020-03-24  71            49.4          41.6          43.1          42.6 #> 4 2020-03-25  88.1          49.1          49.4          41.6          43.1 #> 5 2020-03-26 107.           47.1          49.1          49.4          41.6 #> 6 2020-03-27 139.           40.3          47.1          49.1          49.4  models_dyn <- ohio_covid[ ,1:7] %>%   tidyr::pivot_longer(     cols = contains(\"lead\"),     names_to = \"lead\",     values_to = \"lead_deaths\"   ) %>%   select(date, cases, lead, lead_deaths) %>%   mutate(lead = as.numeric(stringr::str_remove(lead, \"deaths_lead\"))) %>%   tsibble::as_tsibble(index = date, key = lead) %>%   tidyr::drop_na() %>%   tidyr::nest(data = c(date, cases, lead_deaths)) %>%   # Run a regression on lagged cases and date vs deaths   mutate(model = furrr::future_map(data, function(df) {     model(.data = df,           dyn_reg = ARIMA(lead_deaths ~ 1 + cases),           dyn_reg_trend = ARIMA(lead_deaths ~ 1 + cases + trend()),           dyn_reg_quad = ARIMA(lead_deaths ~ 1 + cases + poly(date, 2))     )   }   )) #> Warning: There was 1 warning in `mutate()`. #> ℹ In argument: `model = furrr::future_map(...)`. #> Caused by warning in `sqrt()`: #> ! NaNs produced # shut down workers plan(sequential)  dyn_mod_tbl <- select(models_dyn, -data)  fable_resid_res <- test_fable_resids(dyn_mod_tbl, grp_col = \"lead\", mod_col = \"model\") head(fable_resid_res) #> # A tibble: 6 × 3 #>    lead mod_name     lb_pval #>   <dbl> <chr>          <dbl> #> 1    56 dyn_reg        0.037 #> 2    58 dyn_reg        0.035 #> 3    59 dyn_reg        0.031 #> 4    57 dyn_reg        0.026 #> 5    60 dyn_reg        0.016 #> 6    58 dyn_reg_quad   0.005"},{"path":"https://ercbk.github.io/ebtools/reference/test_lm_resids.html","id":null,"dir":"Reference","previous_headings":"","what":"Autocorrelation tests of the residuals of lm models with various specifications that have been fitted for a grouping variable — test_lm_resids","title":"Autocorrelation tests of the residuals of lm models with various specifications that have been fitted for a grouping variable — test_lm_resids","text":"test_lm_resids() takes nested tibble checks lm model residuals autocorrelation using Breusch-Godfrey Durbin-Watson tests.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/test_lm_resids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Autocorrelation tests of the residuals of lm models with various specifications that have been fitted for a grouping variable — test_lm_resids","text":"","code":"test_lm_resids(mod_tbl, grp_col, mod_col)"},{"path":"https://ercbk.github.io/ebtools/reference/test_lm_resids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Autocorrelation tests of the residuals of lm models with various specifications that have been fitted for a grouping variable — test_lm_resids","text":"mod_tbl tibble grouping variable nested list column list model objects grouping variable value grp_col name grouping variable column mod_col name nested list column lists lm model objects","code":""},{"path":"https://ercbk.github.io/ebtools/reference/test_lm_resids.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Autocorrelation tests of the residuals of lm models with various specifications that have been fitted for a grouping variable — test_lm_resids","text":"unnested tibble columns grouping variable, model names, p-values tests.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/test_lm_resids.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Autocorrelation tests of the residuals of lm models with various specifications that have been fitted for a grouping variable — test_lm_resids","text":"P-values less 0.05 indicate autocorrelation present. p-values round less 0.000, single \"0\" returned.","code":""},{"path":[]},{"path":"https://ercbk.github.io/ebtools/reference/test_lm_resids.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Autocorrelation tests of the residuals of lm models with various specifications that have been fitted for a grouping variable — test_lm_resids","text":"","code":"library(dplyr, warn.conflicts = FALSE)  head(ohio_covid)[ ,1:6] #> # A tibble: 6 × 6 #>   date       cases deaths_lead60 deaths_lead59 deaths_lead58 deaths_lead57 #>   <date>     <dbl>         <dbl>         <dbl>         <dbl>         <dbl> #> 1 2020-03-22  44.9          43.1          42.6          40.6          42.9 #> 2 2020-03-23  56.3          41.6          43.1          42.6          40.6 #> 3 2020-03-24  71            49.4          41.6          43.1          42.6 #> 4 2020-03-25  88.1          49.1          49.4          41.6          43.1 #> 5 2020-03-26 107.           47.1          49.1          49.4          41.6 #> 6 2020-03-27 139.           40.3          47.1          49.1          49.4  models_lm <- ohio_covid %>%   tidyr::pivot_longer(     cols = contains(\"lead\"),     names_to = \"lead\",     values_to = \"lead_deaths\"   ) %>%   mutate(lead = as.numeric(stringr::str_remove(lead, \"deaths_lead\"))) %>%   tidyr::nest(data = c(date, cases, lead_deaths)) %>%   arrange(lead) %>%   mutate(model = purrr::map(data, function(df) {     lm_poly <- lm(lead_deaths ~ cases + poly(date, 3), data = df, na.action = NULL)     lm_poly_log <- lm(log(lead_deaths) ~ log(cases) + poly(date, 3), data = df, na.action = NULL)     lm_quad_st <- lm(lead_deaths ~ cases + poly(date, 3), data = df, na.action = NULL)     lm_quad_log <- lm(log(lead_deaths) ~ log(cases) + poly(date, 3), data = df, na.action = NULL)     lm_ls <- list(lm_quad_st = lm_quad_st, lm_quad_log = lm_quad_log,                   lm_poly = lm_poly, lm_poly_log = lm_poly_log)     return(lm_ls)   }   ))  models_tbl <- select(models_lm, -data) group_var <- \"lead\" model_var <- \"model\"  resid_test_results <- test_lm_resids(models_tbl, group_var, model_var) head(resid_test_results) #> # A tibble: 6 × 4 #>    lead mod_name    bg_pval dw_pval #>   <dbl> <chr>         <dbl>   <dbl> #> 1     0 lm_quad_st        0       0 #> 2     0 lm_quad_log       0       0 #> 3     0 lm_poly           0       0 #> 4     0 lm_poly_log       0       0 #> 5     1 lm_quad_st        0       0 #> 6     1 lm_quad_log       0       0"},{"path":"https://ercbk.github.io/ebtools/reference/tidyeval.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy eval helpers — tidyeval","title":"Tidy eval helpers — tidyeval","text":"sym() creates symbol string syms() creates list symbols character vector. enquo() enquos() delay execution one several function arguments. enquo() returns single quoted expression, like blueprint delayed computation. enquos() returns list quoted expressions. expr() quotes new expression locally. mostly useful build new expressions around arguments captured enquo() enquos(): expr(mean(!!enquo(arg), na.rm = TRUE)). as_name() transforms quoted variable name string. Supplying something else quoted variable name error. unlike as_label() also returns single string supports kind R object input, including quoted function calls vectors. purpose summarise object single label. label often suitable default name. know quoted expression contains (instance expressions captured enquo() variable name, call function, unquoted constant), use as_label(). know quoted simple variable name, like enforce , use as_name(). learn tidy eval use tools, visit https://tidyeval.tidyverse.org Metaprogramming section Advanced R.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/to_js_array.html","id":null,"dir":"Reference","previous_headings":"","what":"Converts data columns to a js array — to_js_array","title":"Converts data columns to a js array — to_js_array","text":"to_js_array() takes tibble grouping column columns combined js array.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/to_js_array.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Converts data columns to a js array — to_js_array","text":"","code":"to_js_array(.data, .grp_var, ..., array_name)"},{"path":"https://ercbk.github.io/ebtools/reference/to_js_array.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Converts data columns to a js array — to_js_array","text":".data tibble; data grouping column columns used create js array column .grp_var grouping column ... columns .data used create js array column array_name string; name newly created js array column","code":""},{"path":"https://ercbk.github.io/ebtools/reference/to_js_array.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Converts data columns to a js array — to_js_array","text":"tibble grouping column js array column","code":""},{"path":"https://ercbk.github.io/ebtools/reference/to_js_array.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Converts data columns to a js array — to_js_array","text":"js array column created list column form <array_name> = list(list(array_var1=var1val1, array_var2 = var2val1, ...), list(array_var1=var1val2, array_var2=var2val2, ...), ...) grouping variable category. like use dataui package along reactable package. dataui still developmental phase requires data js array like format.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/to_js_array.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Converts data columns to a js array — to_js_array","text":"","code":"head(indiana_pos_rate) #> # A tibble: 6 × 3 #>   end_date   msa         pos_rate #>   <date>     <chr>          <dbl> #> 1 2020-04-04 Bloomington   0.112  #> 2 2020-04-11 Bloomington   0.125  #> 3 2020-04-18 Bloomington   0.111  #> 4 2020-04-25 Bloomington   0.0281 #> 5 2020-05-02 Bloomington   0.0232 #> 6 2020-05-09 Bloomington   0.0233  pos_rate_array <- to_js_array(.data = indiana_pos_rate,                               .grp_var = msa,                               end_date, pos_rate,                               array_name = \"posList\")  head(pos_rate_array) #> # A tibble: 6 × 2 #>   msa                          posList          #>   <chr>                        <list>           #> 1 Bloomington                  <named list [1]> #> 2 Columbus                     <named list [1]> #> 3 Fort Wayne                   <named list [1]> #> 4 Elkhart-Goshen               <named list [1]> #> 5 Chicago-Naperville-Elgin     <named list [1]> #> 6 Indianapolis-Carmel-Anderson <named list [1]>"},{"path":"https://ercbk.github.io/ebtools/reference/us_regional_cases.html","id":null,"dir":"Reference","previous_headings":"","what":"US Regional COVID-19 Positive Cases Data Set — us_regional_cases","title":"US Regional COVID-19 Positive Cases Data Set — us_regional_cases","text":"Contains rolling 7-day average COVID-19 cases US regional areas April 1st, 2020 February 13th, 2021.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/us_regional_cases.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"US Regional COVID-19 Positive Cases Data Set — us_regional_cases","text":"","code":"us_regional_cases"},{"path":"https://ercbk.github.io/ebtools/reference/us_regional_cases.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"US Regional COVID-19 Positive Cases Data Set — us_regional_cases","text":"object class tbl_df (inherits tbl, data.frame) 1276 rows 3 columns.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/us_regional_cases.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"US Regional COVID-19 Positive Cases Data Set — us_regional_cases","text":"Data collected Delphi Research Group.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/us_regional_cases.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"US Regional COVID-19 Positive Cases Data Set — us_regional_cases","text":"","code":"head(us_regional_cases) #> # A tibble: 6 × 3 #>   date       region    reg_sev_day_cases #>   <date>     <chr>                 <dbl> #> 1 2020-04-01 midwest               2884. #> 2 2020-04-01 northeast            13252. #> 3 2020-04-01 south                 3797. #> 4 2020-04-01 west                  2258. #> 5 2020-04-02 midwest               3128. #> 6 2020-04-02 northeast            14121."},{"path":"https://ercbk.github.io/ebtools/reference/us_regional_deaths.html","id":null,"dir":"Reference","previous_headings":"","what":"US Regional COVID-19 Deaths Data Set — us_regional_deaths","title":"US Regional COVID-19 Deaths Data Set — us_regional_deaths","text":"Contains rolling 7-day average COVID-19 deaths US regional areas April 1st, 2020 February 13th, 2021.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/us_regional_deaths.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"US Regional COVID-19 Deaths Data Set — us_regional_deaths","text":"","code":"us_regional_deaths"},{"path":"https://ercbk.github.io/ebtools/reference/us_regional_deaths.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"US Regional COVID-19 Deaths Data Set — us_regional_deaths","text":"object class tbl_df (inherits tbl, data.frame) 1276 rows 3 columns.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/us_regional_deaths.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"US Regional COVID-19 Deaths Data Set — us_regional_deaths","text":"Data collected Delphi Research Group.","code":""},{"path":"https://ercbk.github.io/ebtools/reference/us_regional_deaths.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"US Regional COVID-19 Deaths Data Set — us_regional_deaths","text":"","code":"head(us_regional_deaths) #> # A tibble: 6 × 3 #>   date       region    reg_sev_day_deaths #>   <date>     <chr>                  <dbl> #> 1 2020-04-01 midwest                 99.0 #> 2 2020-04-01 northeast              490.  #> 3 2020-04-01 south                   93.3 #> 4 2020-04-01 west                    65.1 #> 5 2020-04-02 midwest                117.  #> 6 2020-04-02 northeast              595."},{"path":"https://ercbk.github.io/ebtools/news/index.html","id":"ebtools-0009000","dir":"Changelog","previous_headings":"","what":"ebtools 0.0.0.9000","title":"ebtools 0.0.0.9000","text":"Added NEWS.md file track changes package. Added create_dtw_grids, dtw_dist_gridsearch, test_fable_resids, test_lm_resids, prewhitened_ccf, to_js_array","code":""}]
